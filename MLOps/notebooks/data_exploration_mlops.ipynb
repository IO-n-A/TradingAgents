{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps Data Exploration Notebook\n",
    "\n",
    "This notebook is intended for exploratory data analysis (EDA) related to MLOps tasks, such as:\n",
    "- Investigating raw data characteristics.\n",
    "- Analyzing processed data and features.\n",
    "- Exploring model predictions and errors.\n",
    "- Debugging data-related issues in pipelines.\n",
    "\n",
    "It complements the automated monitoring dashboards by providing a flexible environment for ad-hoc analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "# Configure logging (optional for notebooks, but can be useful)\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define base paths (adjust if your notebook is in a different location relative to project root)\n",
    "# Assuming the notebook is run from the FinAI_algo root or paths are relative to it.\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '../..')) # Adjust if notebook is deeper\n",
    "MLOPS_CONFIG_DIR = os.path.join(PROJECT_ROOT, 'MLOps/config')\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data') # Assuming data is in FinAI_algo/data/\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, 'MLOps/results')\n",
    "\n",
    "logger.info(f\"PROJECT_ROOT set to: {PROJECT_ROOT}\")\n",
    "logger.info(f\"MLOPS_CONFIG_DIR set to: {MLOPS_CONFIG_DIR}\")\n",
    "logger.info(f\"DATA_DIR set to: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration Files (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml_config(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        logger.info(f\"Successfully loaded config: {file_path}\")\n",
    "        return config\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Config file not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading config {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example: Load global vars\n",
    "# global_vars_path = os.path.join(MLOPS_CONFIG_DIR, 'common/global_vars.yaml')\n",
    "# global_vars = load_yaml_config(global_vars_path)\n",
    "# if global_vars:\n",
    "#     print(\"Global Vars:\", global_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data\n",
    "\n",
    "Load datasets relevant to your exploration. This could be:\n",
    "- Raw data from `data/raw/`\n",
    "- Processed data from `data/processed/` (potentially DVC-tracked)\n",
    "- Backtesting results from `MLOps/results/backtesting/`\n",
    "- Model predictions or outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load processed data (replace with actual file name)\n",
    "# processed_data_path = os.path.join(DATA_DIR, 'processed/sample_processed_data.csv')\n",
    "# try:\n",
    "#     df_processed = pd.read_csv(processed_data_path, parse_dates=['date'])\n",
    "#     logger.info(f\"Loaded processed data from {processed_data_path}. Shape: {df_processed.shape}\")\n",
    "#     print(df_processed.head())\n",
    "# except FileNotFoundError:\n",
    "#     logger.error(f\"Processed data file not found: {processed_data_path}. Please ensure it exists.\")\n",
    "#     df_processed = pd.DataFrame() # Create empty df to avoid errors later\n",
    "\n",
    "# Create dummy data for demonstration if files don't exist\n",
    "if 'df_processed' not in locals() or df_processed.empty:\n",
    "    logger.warning(\"Creating dummy processed data for notebook demonstration.\")\n",
    "    dates = pd.date_range(start=\"2023-01-01\", periods=100, freq=\"B\")\n",
    "    df_processed = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'tic': np.random.choice(['AAPL', 'MSFT'], 100),\n",
    "        'price': np.random.rand(100) * 100 + 100,\n",
    "        'volume': np.random.randint(100000, 1000000, 100),\n",
    "        'sentiment_score': np.random.normal(0, 0.3, 100),\n",
    "        'feature1_tech': np.random.rand(100),\n",
    "        'feature2_tech': np.random.rand(100)\n",
    "    })\n",
    "    print(\"Dummy Processed Data Head:\")\n",
    "    print(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Perform your EDA here. Examples:\n",
    "- Summary statistics\n",
    "- Data distributions (histograms, density plots)\n",
    "- Time series plots\n",
    "- Correlation analysis\n",
    "- Missing value analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_processed.empty:\n",
    "    print(\"\\n--- Summary Statistics ---\")\n",
    "    print(df_processed.describe(include='all'))\n",
    "\n",
    "    print(\"\\n--- Missing Values ---\")\n",
    "    print(df_processed.isnull().sum())\n",
    "\n",
    "    # Plot distribution of a numerical feature\n",
    "    if 'price' in df_processed.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(df_processed['price'], kde=True)\n",
    "        plt.title('Distribution of Price')\n",
    "        plt.xlabel('Price')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "    # Plot sentiment score distribution\n",
    "    if 'sentiment_score' in df_processed.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(df_processed['sentiment_score'], kde=True)\n",
    "        plt.title('Distribution of Sentiment Score')\n",
    "        plt.xlabel('Sentiment Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "        \n",
    "    # Time series plot for a ticker\n",
    "    if 'tic' in df_processed.columns and 'price' in df_processed.columns and 'date' in df_processed.columns:\n",
    "        aapl_data = df_processed[df_processed['tic'] == 'AAPL']\n",
    "        if not aapl_data.empty:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(aapl_data['date'], aapl_data['price'])\n",
    "            plt.title('AAPL Price Over Time')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Price')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    logger.warning(\"df_processed is empty. Skipping EDA plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Specific MLOps Investigations\n",
    "\n",
    "### 5.1 Data Drift Investigation\n",
    "If `check_data_drift.py` indicated drift, you can load the reference and current datasets here for a deeper dive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load reference and current data for drift analysis\n",
    "# ref_data_path = os.path.join(DATA_DIR, 'processed/reference_training_data.csv')\n",
    "# current_data_path = os.path.join(DATA_DIR, 'processed/recent_production_batch.csv')\n",
    "\n",
    "# try:\n",
    "#     df_ref = pd.read_csv(ref_data_path)\n",
    "#     df_curr = pd.read_csv(current_data_path)\n",
    "#     logger.info(\"Loaded reference and current data for drift investigation.\")\n",
    "      # Perform comparative analysis, e.g., plot distributions side-by-side\n",
    "#     if 'feature1_tech' in df_ref.columns and 'feature1_tech' in df_curr.columns:\n",
    "#         plt.figure(figsize=(12, 6))\n",
    "#         sns.kdeplot(df_ref['feature1_tech'], label='Reference Data', fill=True)\n",
    "#         sns.kdeplot(df_curr['feature1_tech'], label='Current Data', fill=True)\n",
    "#         plt.title('Distribution Comparison for feature1_tech')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "# except FileNotFoundError:\n",
    "#     logger.warning(\"Reference or current data for drift not found. Skipping drift investigation section.\")\n",
    "logger.info(\"Placeholder for Data Drift Investigation section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Model Prediction Analysis\n",
    "Load model predictions and actuals to analyze errors, biases, or specific scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load predictions\n",
    "# predictions_path = os.path.join(RESULTS_DIR, 'model_predictions/sentiment_model_preds_on_eval.csv')\n",
    "# try:\n",
    "#     df_preds = pd.read_csv(predictions_path)\n",
    "#     logger.info(f\"Loaded predictions from {predictions_path}\")\n",
    "      # Analyze prediction errors, confusion matrix, etc.\n",
    "# except FileNotFoundError:\n",
    "#     logger.warning(f\"Predictions file not found: {predictions_path}. Skipping prediction analysis.\")\n",
    "logger.info(\"Placeholder for Model Prediction Analysis section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions & Next Steps\n",
    "\n",
    "Summarize findings from the exploration and outline any actions or further investigations needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Notebook execution placeholder finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}