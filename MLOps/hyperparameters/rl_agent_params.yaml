# MLOps/hyperparameters/rl_agent_params.yaml
# This file stores configurations for Reinforcement Learning (RL) agents.
# These configurations can be loaded by the DRLAgent or training scripts.

# Default parameters for PPO (Proximal Policy Optimization)
ppo_default:
  model_name: "ppo"
  policy: "MlpPolicy" # Can be MlpPolicy, CnnPolicy, MultiInputPolicy
  policy_kwargs: null # e.g., dict(net_arch=[dict(pi=[128, 128], vf=[128, 128])])
  model_kwargs: # Refer to stable_baselines3.common.ppo.PPO and finrl.config.PPO_PARAMS
    n_steps: 2048
    ent_coef: 0.01
    learning_rate: 0.00025 # Can be a float or a schedule
    batch_size: 64 # PPO default is 64
    # gamma: 0.99
    # gae_lambda: 0.95
    # clip_range: 0.2
    # n_epochs: 10
    # vf_coef: 0.5
  verbose: 1
  seed: null # Set for reproducibility
  tensorboard_log: "logs/tensorboard/ppo/"

# Default parameters for A2C (Advantage Actor Critic)
a2c_default:
  model_name: "a2c"
  policy: "MlpPolicy"
  policy_kwargs: null
  model_kwargs: # Refer to stable_baselines3.common.a2c.A2C and finrl.config.A2C_PARAMS
    n_steps: 5
    ent_coef: 0.005 # FinRL example, SB3 default is 0.0
    learning_rate: 0.0007 # Can be a float or a schedule
    # gamma: 0.99
    # gae_lambda: 1.0
    # vf_coef: 0.5
    # max_grad_norm: 0.5
  verbose: 1
  seed: null
  tensorboard_log: "logs/tensorboard/a2c/"

# Default parameters for DDPG (Deep Deterministic Policy Gradient)
ddpg_default:
  model_name: "ddpg"
  policy: "MlpPolicy"
  policy_kwargs: null
  model_kwargs: # Refer to stable_baselines3.common.ddpg.DDPG and finrl.config.DDPG_PARAMS
    buffer_size: 10000 # FinRL example, SB3 default 1_000_000
    learning_rate: 0.0005 # FinRL example, SB3 default 0.001
    batch_size: 64 # FinRL example, SB3 default 100
    action_noise: "normal" # Example: normal, ornstein_uhlenbeck
    # gamma: 0.99
    # tau: 0.005
  verbose: 1
  seed: null
  tensorboard_log: "logs/tensorboard/ddpg/"

# Default parameters for SAC (Soft Actor-Critic)
sac_default:
  model_name: "sac"
  policy: "MlpPolicy"
  policy_kwargs: null
  model_kwargs: # Refer to stable_baselines3.common.sac.SAC and finrl.config.SAC_PARAMS
    batch_size: 128 # FinRL example, SB3 default 256
    buffer_size: 100000 # FinRL example, SB3 default 1_000_000
    learning_rate: 0.0001 # FinRL example, SB3 default 0.0003
    learning_starts: 100
    ent_coef: "auto_0.1"
    # gamma: 0.99
    # tau: 0.005
  verbose: 1
  seed: null
  tensorboard_log: "logs/tensorboard/sac/"

# Default parameters for TD3 (Twin Delayed Deep Deterministic Policy Gradient)
td3_default:
  model_name: "td3"
  policy: "MlpPolicy"
  policy_kwargs: null
  model_kwargs: # Refer to stable_baselines3.common.td3.TD3 and finrl.config.TD3_PARAMS
    batch_size: 64 # FinRL example, SB3 default 100
    buffer_size: 100000 # FinRL example, SB3 default 1_000_000
    learning_rate: 0.0008 # FinRL example, SB3 default 0.001
    action_noise: "normal" # Example: normal, ornstein_uhlenbeck
    # gamma: 0.99
    # tau: 0.005
  verbose: 1
  seed: null
  tensorboard_log: "logs/tensorboard/td3/"

# Add other RL agent configurations as needed.
# For policy_kwargs, refer to stable_baselines3 documentation for specific policy network architectures.
# Example:
# ppo_custom_arch:
#   model_name: "ppo"
#   policy: "MlpPolicy"
#   policy_kwargs:
#     net_arch: [dict(pi=[256, 256], vf=[256, 256])]
#     activation_fn: "ReLU" # or "Tanh"
#   model_kwargs:
#     n_steps: 2048
#     learning_rate: 0.0003
#   verbose: 1
#   seed: 42
#   tensorboard_log: "logs/tensorboard/ppo_custom/"