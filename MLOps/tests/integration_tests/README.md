# MLOps Integration Tests

This directory is designated for integration tests within the MLOps system of the FinAI_algo project. Integration tests focus on verifying the interactions and data flow between different components or stages of the MLOps pipelines.

## Objective

The primary goal of integration tests is to ensure that independently developed MLOps components (e.g., data ingestion, feature engineering, model training) work together correctly as part of a larger workflow. They help catch issues related to:

*   **Interface Mismatches:** Incorrect data schemas, file formats, or API contracts between components.
*   **Data Handoff:** Problems in how data is passed from one pipeline stage to the next (e.g., via DVC-tracked files, databases, or in-memory objects if part of a single script).
*   **Configuration Propagation:** Ensuring that configurations are correctly passed and interpreted across different parts of a multi-stage process.
*   **End-to-End Pipeline Runs (Simplified):** Testing a simplified version of an end-to-end workflow with small, controlled datasets to verify basic connectivity and functionality.

## Scope of Integration Tests

*   **Testing interactions between two or more pipeline scripts:**
    *   Example: Does the output of `data_ingestion_pipeline.py` (raw data CSVs) serve as valid input for `build_features.py`?
    *   Example: Can `train_rl_agent.py` correctly load and use the processed data generated by `build_features.py`?
*   **Testing data flow through DVC:**
    *   Example: A test that runs data ingestion, `dvc add`s the output, then runs feature engineering which `dvc pull`s (or directly accesses) that data, and verifies the feature engineering output.
*   **Testing interactions with external services (with mocks):**
    *   Example: If a pipeline component interacts with MLflow, an integration test might use a local MLflow instance or mock the MLflow API to verify logging behavior.

## What Not to Test Here (Typically)

*   **Exhaustive unit logic:** Detailed internal logic of individual functions within a component should be covered by unit tests in `MLOps/tests/pipeline_tests/`.
*   **Full-scale performance or stress tests:** These require a different setup and are usually conducted separately.
*   **Interactions with real, live external services (unless absolutely necessary and carefully managed):** Prefer using mocks or test doubles for external dependencies to ensure test reliability and avoid side effects.

## Example Test Scenarios

1.  **Data Ingestion to Feature Engineering:**
    *   Run `data_ingestion_pipeline.py` with minimal configuration to produce small sample raw data files.
    *   Run `dvc add` on these files.
    *   Run `build_features.py` using these DVC-tracked raw data files.
    *   Assert that `build_features.py` completes successfully and produces an output file with an expected schema or number of rows/columns.

2.  **Feature Engineering to RL Agent Training (Environment Setup):**
    *   Use a pre-existing small, DVC-tracked processed data file (output of feature engineering).
    *   Have `train_rl_agent.py` (or a test wrapper around its environment setup logic) attempt to initialize the `StockTradingEnv` using this data and relevant configurations.
    *   Assert that the environment initializes without errors and that the observation/action spaces have expected dimensions.

3.  **MLflow Logging Integration:**
    *   Run a simplified version of a training pipeline script (`train_sentiment_model.py` or `train_rl_agent.py`) that is configured to log to a local MLflow instance (e.g., `mlflow.set_tracking_uri("sqlite:///./test_mlflow.db")`).
    *   After the script runs, use the MLflow client API to query the local MLflow database and verify that expected parameters, metrics, or artifacts were logged.

## Test Structure and Tools

*   **Framework:** `pytest` is recommended.
*   **Fixtures:** Use `pytest` fixtures to set up preconditions, such as creating temporary data directories, sample configuration files, or initializing mock services.
*   **Temporary Files/Directories:** Use `tmp_path` fixture from `pytest` or Python's `tempfile` module to manage test-specific files that are cleaned up automatically.
*   **Mocking:** Use libraries like `unittest.mock` (built-in) or `pytest-mock` to create mocks for external dependencies or complex internal components not under test.
*   **DVC Integration:** Tests involving DVC might require scripting DVC commands (`subprocess.run(['dvc', 'add', ...])`) or using the DVC Python API if suitable for the test scenario. Care must be taken to manage the DVC state within the test environment (e.g., initializing a temporary DVC repository for the test).

## Running Integration Tests

```bash
pytest MLOps/tests/integration_tests/
```

Integration tests are generally slower to run than unit tests but provide valuable confidence in the overall system's cohesion. They are a critical part of a robust MLOps testing strategy.